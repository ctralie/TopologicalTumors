{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c94223",
   "metadata": {},
   "source": [
    "# Precompute all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eb2120",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pickle\n",
    "import nibabel as nib\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure # For marching cubes\n",
    "import polyscope as ps # For mesh display\n",
    "from persim import plot_diagrams, PersistenceImager\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import skimage\n",
    "import skimage.io\n",
    "sys.path.append(\"../src\")\n",
    "import glob\n",
    "from geomstats import *\n",
    "from topostats import *\n",
    "from kernels import *\n",
    "from utils3d import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a70dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dictionary(metadata_path):\n",
    "    df = pd.read_csv(metadata_path)\n",
    "    data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        patient_id = row[\"ID\"]\n",
    "        patient_id = \"M-0\".join(patient_id.split(\"M-\")) #File paths have an extra 0 in ID\n",
    "        data[patient_id] = row\n",
    "    del data[\"UCSF-PDGM-0541\"] # Skip Patient 541 because segmentation file is empty\n",
    "    return data\n",
    "\n",
    "def argsort(seq):\n",
    "    return np.array(sorted(range(len(seq)), key=seq.__getitem__), dtype=int)\n",
    "\n",
    "\n",
    "\n",
    "metadata_path = \"../Data/UCSF-PDGM-metadata_v2.csv\"\n",
    "data = load_dictionary(metadata_path) \n",
    "for patient in list(data.keys()):\n",
    "    if not \"Glio\" in data[patient][\"Final pathologic diagnosis (WHO 2021)\"]:\n",
    "        del data[patient]\n",
    "\n",
    "\n",
    "all_patients = {}\n",
    "\n",
    "## Step 1: Enumerate UCSF dataset\n",
    "all_data_path = \"../Data/UCSF-PDGM-v3\"\n",
    "for patient in os.listdir(all_data_path):\n",
    "    patient_folder_path = os.path.join(all_data_path, patient)\n",
    "    patient = patient[:-6]\n",
    "\n",
    "    tumor_seg_path = patient_folder_path + \"/\" + patient_folder_path[-20:-6] + \"_tumor_segmentation.nii.gz\"\n",
    "    if os.path.exists(tumor_seg_path) and patient in data:\n",
    "        all_patients[patient] = {\"tumor_seg_path\":tumor_seg_path}\n",
    "\n",
    "## Step 2: Enumerate TCGA dataset\n",
    "all_data_path = \"../Data/Pre-operative_TCGA_GBM_NIfTI_and_Segmentations\"\n",
    "for patient in os.listdir(all_data_path):\n",
    "    patient_folder_path = os.path.join(all_data_path, patient)\n",
    "    tumor_seg_path = glob.glob(patient_folder_path + os.path.sep + \"*GlistrBoost_ManuallyCorrected*\")\n",
    "    if len(tumor_seg_path) > 0:\n",
    "        all_patients[patient] = {\"tumor_seg_path\":tumor_seg_path[0]}\n",
    "\n",
    "## Step 3: Enumerate Penn dataset\n",
    "files = glob.glob(\"../Data/PennDataset/images_segm/*\") + glob.glob(\"../Data/PennDataset/automated_segm/*\")\n",
    "for path in files:\n",
    "    patient = path.split(\"/\")[-1].split(\"_11_segm.nii.gz\")[0]\n",
    "    all_patients[patient] = {\"tumor_seg_path\":path}\n",
    "\n",
    "        \n",
    "print(\"len(all_patients)\", len(all_patients))\n",
    "## Step 4: Load masks and extract levelsets\n",
    "iso_names = {1:\"Necrotic\", 2:\"Edema\", 4:\"Main Tumor\"} # What the labels actually mean\n",
    "iso_levels = [2, 4, 1] # Column order of the labels\n",
    "tumor_types = [\"Edema\", \"Main Tumor\", \"Necrotic\"]\n",
    "\n",
    "for i, patient in enumerate(all_patients.values()):\n",
    "    if i%10 == 0:\n",
    "        print(\".\", end=\"\")\n",
    "    tumor_seg_path = patient[\"tumor_seg_path\"]\n",
    "    tumor_seg_nifti = nib.load(tumor_seg_path)\n",
    "    tumor_seg_mat = tumor_seg_nifti.get_fdata()\n",
    "    \n",
    "    for k, level in enumerate(iso_levels):\n",
    "        binary = tumor_seg_mat==level\n",
    "        level_name = iso_names[level]\n",
    "        B = crop_binary_volume(binary)\n",
    "        patient[\"B{}\".format(level_name)] = B\n",
    "        X = binary_volume_2coords(binary)\n",
    "        patient[\"X{}\".format(level_name)] = X\n",
    "\n",
    "data = all_patients\n",
    "to_delete = []\n",
    "for p in data:\n",
    "    if not \"XEdema\" in data[p].keys():\n",
    "        to_delete.append(p)\n",
    "print(to_delete)\n",
    "for p in to_delete:\n",
    "    del data[p]\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4662a73b",
   "metadata": {},
   "source": [
    "## Total Persistences: Alpha\n",
    "\n",
    "(TODO Later: Grab and sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aec589",
   "metadata": {},
   "outputs": [],
   "source": [
    "persistence_cutoff = 1\n",
    "\n",
    "out_dir = \"../preprocessed/alphatotal\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "for p in data:\n",
    "    fileout = \"{}/{}.pkl\".format(out_dir, p)\n",
    "    if os.path.exists(fileout):\n",
    "        continue\n",
    "    total = []\n",
    "    for i, name in enumerate(iso_levels):\n",
    "        name = iso_names[name]\n",
    "        X = data[p][\"X{}\".format(name)]\n",
    "        PDs = get_alpha_filtration_3d(X)\n",
    "        PDs = remove_infinite(PDs)\n",
    "        totali = []\n",
    "        for k in range(len(PDs)):\n",
    "            Ik = PDs[k]\n",
    "            if Ik.size > 0:\n",
    "                PDs[k] = Ik[Ik[:, 1]-Ik[:, 0] > persistence_cutoff, :]\n",
    "                totali.append(np.sum(PDs[k][:, 1]-PDs[k][:, 0]))\n",
    "            else:\n",
    "                totali.append(0)\n",
    "        total.append(totali)\n",
    "    total = np.array(total)\n",
    "    pickle.dump({\"x\":total}, open(fileout, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86be389b",
   "metadata": {},
   "source": [
    "## Total Persistences: Cubical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85721b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_kernels = []\n",
    "all_kernels.append([gauss3d(w=3), gauss3d(w=5), gauss3d(w=7), laplacian3d(w=3), laplacian3d(w=5), laplacian3d(w=7)])\n",
    "randpath = \"../preprocessed/cubicalrandtotal/kernels.pkl\"\n",
    "if not os.path.exists(randpath):\n",
    "    all_kernels.append(get_random_3d_kernels(5, 10))\n",
    "else:\n",
    "    all_kernels.append(pickle.load(open(randpath, \"rb\"))[\"kernels\"])\n",
    "\n",
    "all_dirs = [\"../preprocessed/cubicaltotal\", \"../preprocessed/cubicalrandtotal\"]\n",
    "\n",
    "\n",
    "for kernels, out_dir in zip(all_kernels, all_dirs):\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.mkdir(out_dir)\n",
    "    pickle.dump({\"kernels\":kernels}, open(\"{}/kernels.pkl\".format(out_dir), \"wb\"))\n",
    "\n",
    "    for p in data:\n",
    "        fileout = \"{}/{}.pkl\".format(out_dir, p)\n",
    "        if os.path.exists(fileout):\n",
    "            continue\n",
    "        total = []\n",
    "        for i, name in enumerate(iso_levels):\n",
    "            name = iso_names[level]\n",
    "            B = data[p][\"B{}\".format(name)]\n",
    "            PDs = []\n",
    "            for kernel in kernels:\n",
    "                PDs += get_binary_kernel_cubical_filtration(B, kernel)\n",
    "            PDs = remove_infinite(PDs)\n",
    "            for I in PDs:\n",
    "                if I.size > 0:\n",
    "                    total.append(np.sum(I[:, 1]-I[:, 0]))\n",
    "                else:\n",
    "                    total.append(0)\n",
    "        total = np.array(total)\n",
    "        pickle.dump({\"x\":total}, open(fileout, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761177cf",
   "metadata": {},
   "source": [
    "## Fractal Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b3ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmin = 6\n",
    "dmax = 12\n",
    "\n",
    "out_dir = \"../preprocessed/fractaldim\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "for p in data:\n",
    "    fileout = \"{}/{}.pkl\".format(out_dir, p)\n",
    "    if os.path.exists(fileout):\n",
    "        continue\n",
    "    dims = np.zeros(len(tumor_types))\n",
    "    for i, tumor_type in enumerate(tumor_types):\n",
    "        B = data[p][\"B{}\".format(tumor_type)]\n",
    "        if np.sum(B) > 1: # There is at least one edge\n",
    "            dims[i] = get_correlation_dimension_grid(B, dmin, dmax)\n",
    "    pickle.dump({\"x\":dims}, open(fileout, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517e22fe",
   "metadata": {},
   "source": [
    "## Shape Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403ffcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_max = 75\n",
    "n_shells = 20\n",
    "\n",
    "out_dir = \"../preprocessed/shapehist\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "for p in data:\n",
    "    fileout = \"{}/{}.pkl\".format(out_dir, p)\n",
    "    if os.path.exists(fileout):\n",
    "        continue\n",
    "    hists = np.array([])\n",
    "    for tumor_type in tumor_types:\n",
    "        X = data[p][\"X{}\".format(tumor_type)]\n",
    "        h = get_shape_hist(X, n_shells=n_shells, r_max=r_max)\n",
    "        if hists.size == 0:\n",
    "            hists = h\n",
    "        else:\n",
    "            hists = np.concatenate((hists, h))\n",
    "    pickle.dump({\"x\":hists}, open(fileout, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cd8d9f",
   "metadata": {},
   "source": [
    "## Shape Shell Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3698d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "r_max = 75\n",
    "n_shells = 20\n",
    "subdiv = 1 # How many times to subdivide the sphere for sector points\n",
    "\n",
    "out_dir = \"../preprocessed/shapeshellhist\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "for p in data:\n",
    "    fileout = \"{}/{}.pkl\".format(out_dir, p)\n",
    "    if os.path.exists(fileout):\n",
    "        continue\n",
    "    hists = np.array([])\n",
    "    for tumor_type in tumor_types:\n",
    "        X = data[p][\"X{}\".format(tumor_type)]\n",
    "        h = get_shape_shell_hist(X, n_shells=n_shells, r_max=r_max, subdiv=subdiv)\n",
    "        print(h.shape)\n",
    "        if hists.size == 0:\n",
    "            hists = h\n",
    "        else:\n",
    "            hists = np.concatenate((hists, h))\n",
    "    pickle.dump({\"x\":hists}, open(fileout, \"wb\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861923e6",
   "metadata": {},
   "source": [
    "## Shape PCA Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33eccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_max = 75\n",
    "n_shells = 10\n",
    "\n",
    "out_dir = \"../preprocessed/shapehistpca\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "for p in data:\n",
    "    fileout = \"{}/{}.pkl\".format(out_dir, p)\n",
    "    if os.path.exists(fileout):\n",
    "        continue\n",
    "    hists = np.array([])\n",
    "    for tumor_type in tumor_types:\n",
    "        X = data[p][\"X{}\".format(tumor_type)]\n",
    "        h = get_shape_pca_hist(X, n_shells=n_shells, r_max=r_max)\n",
    "        if hists.size == 0:\n",
    "            hists = h\n",
    "        else:\n",
    "            hists = np.concatenate((hists, h))\n",
    "    pickle.dump({\"x\":hists}, open(fileout, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e658e8",
   "metadata": {},
   "source": [
    "## D2 Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cae4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_max = 75\n",
    "n_bins = 40 # Number of bins in the histogram between [0, d_max]\n",
    "n_samples = 10000 # Number of random samples\n",
    "\n",
    "out_dir = \"../preprocessed/d2\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "for p in data:\n",
    "    fileout = \"{}/{}.pkl\".format(out_dir, p)\n",
    "    if os.path.exists(fileout):\n",
    "        continue\n",
    "    hists = np.array([])\n",
    "    for tumor_type in tumor_types:\n",
    "        X = data[p][\"X{}\".format(tumor_type)]\n",
    "        h = get_d2_hist(X, d_max=r_max*2, n_bins=n_bins, n_samples=n_samples)\n",
    "        if hists.size == 0:\n",
    "            hists = h\n",
    "        else:\n",
    "            hists = np.concatenate((hists, h))\n",
    "    pickle.dump({\"x\":hists}, open(fileout, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44256ea2",
   "metadata": {},
   "source": [
    "## Spin Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83b8e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "r_max = 75\n",
    "n_angles = 50\n",
    "dim = 64\n",
    "\n",
    "out_dir = \"../preprocessed/spinimages\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "for p in data:\n",
    "    fileout = \"{}/{}.pkl\".format(out_dir, p)\n",
    "    if os.path.exists(fileout):\n",
    "        continue\n",
    "    imgs = np.array([])\n",
    "    for tumor_type in tumor_types:\n",
    "        X = data[p][\"X{}\".format(tumor_type)]\n",
    "        img = get_spin_image(X, n_angles, r_max, dim).flatten()\n",
    "        if imgs.size == 0:\n",
    "            imgs = img\n",
    "        else:\n",
    "            imgs = np.concatenate((imgs, img))\n",
    "    pickle.dump({\"x\":imgs}, open(fileout, \"wb\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea711767",
   "metadata": {},
   "source": [
    "## Connected Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ed91e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_components = 10\n",
    "\n",
    "out_dir = \"../preprocessed/connectedcomponents\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "for p in data:\n",
    "    fileout = \"{}/{}.pkl\".format(out_dir, p)\n",
    "    if os.path.exists(fileout):\n",
    "        continue\n",
    "    all_counts = np.zeros(max_components*len(iso_names))\n",
    "    for i, tumor_type in enumerate(tumor_types):\n",
    "        B = data[p][\"B{}\".format(tumor_type)]\n",
    "        B = crop_binary_volume(B)\n",
    "        if B.size > 0:\n",
    "            labels = label_volume_components(B, cluster_cutoff=1)\n",
    "            counts = sorted(get_label_counts(labels).flatten())[::-1]\n",
    "            i1 = i*max_components\n",
    "            i2 = min((i+1)*max_components, i1+len(counts))\n",
    "            all_counts[i1:i2] = counts[0:(i2-i1)]\n",
    "    pickle.dump({\"x\":all_counts}, open(fileout, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20c7f21",
   "metadata": {},
   "source": [
    "## Region Voxel Counts\n",
    "\n",
    "(Can usually be inferred from connected component counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92608686",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"../preprocessed/voxelcount\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "for p in data:\n",
    "    fileout = \"{}/{}.pkl\".format(out_dir, p)\n",
    "    if os.path.exists(fileout):\n",
    "        continue\n",
    "    counts = np.zeros(len(tumor_types))\n",
    "    for i, tumor_type in enumerate(tumor_types):\n",
    "        B = data[p][\"B{}\".format(tumor_type)]\n",
    "        counts[i] = np.sum(B)\n",
    "    pickle.dump({\"x\":counts}, open(fileout, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edadbd87",
   "metadata": {},
   "source": [
    "# Condense Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2349073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import condense_dataset\n",
    "features = [\"alphatotal\", \"connectedcomponents\", \"cubicalrandtotal\", \"cubicaltotal\", \n",
    "            \"d2\", \"shapehist\", \"shapehistpca\", \"voxelcount\", \"fractaldim\"]\n",
    "data = {}\n",
    "for feature in features:\n",
    "    X, IDs = condense_dataset(\"../preprocessed/{}\".format(feature), metadata_path)\n",
    "    pickle.dump({\"X\":X, \"IDs\":IDs}, open(\"../preprocessed/{}.pkl\".format(feature), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21353030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
